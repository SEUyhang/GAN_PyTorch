{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import  save_image\n",
    "import torch.autograd\n",
    "from torch.autograd import Variable\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据并使用PCA进行降维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA对ds3降维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对real0降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取npy文件中的内容到numpy数组中\n",
    "import numpy as np\n",
    "data = np.load(\"/new_data/yhang/GNSS/Complex/complex_ds3/complex_real0_ds3.npy\")\n",
    "data.shape  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用PCA降维\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 1000)\n",
    "pca.fit(data)\n",
    "data_pca = pca.transform(data)\n",
    "data_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重命名\n",
    "ds3_real0 = data_pca\n",
    "np.save(\"/new_data/yhang/GNSS/PCA/ds3/ds3_PCA_real0.npy\", ds3_real0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对real1降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 1000)\n",
    "data = np.load(\"/new_data/yhang/GNSS/Complex/complex_ds3/complex_real1_ds3.npy\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据切割不然内存会爆\n",
    "len = data.shape[0]\n",
    "ds3_real10 = data[0: len // 2, :]\n",
    "ds3_real11 = data[len // 2: len, :]\n",
    "print(ds3_real10.shape, ds3_real11.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(ds3_real10)\n",
    "ds3_real10_pca = pca.transform(ds3_real10)\n",
    "ds3_real10_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(ds3_real11)\n",
    "ds3_real11_pca = pca.transform(ds3_real11)\n",
    "ds3_real11_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds3_real1 = np.vstack((ds3_real10_pca, ds3_real11_pca))\n",
    "print(ds3_real1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"/new_data/yhang/GNSS/PCA/ds3/ds3_PCA_real1.npy\", ds3_real1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对spoof0降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 1000)\n",
    "ds3_spoof0 = np.load(\"/new_data/yhang/GNSS/Complex/complex_ds3/complex_spoof0_ds3.npy\")\n",
    "len = ds3_spoof0.shape[0]\n",
    "ds3_spoof00 = ds3_spoof0[0:len // 2, :]\n",
    "ds3_spoof01 = ds3_spoof0[len // 2:, :]\n",
    "pca.fit(ds3_spoof00)\n",
    "ds3_spoof00 = pca.transform(ds3_spoof00)\n",
    "ds3_spoof00.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(ds3_spoof01)\n",
    "ds3_spoof01 = pca.transform(ds3_spoof01)\n",
    "ds3_spoof01.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并ds3_spoof0和ds3_spoof1为一个数组\n",
    "ds3_spoof0 = np.vstack((ds3_spoof00, ds3_spoof01))\n",
    "ds3_spoof0.shape\n",
    "np.save(\"/new_data/yhang/GNSS/PCA/ds3/ds3_PCA_spoof0.npy\", ds3_spoof0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import torch.autograd\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import torch\n",
    "\n",
    "batch_size = 128 #一批128个\n",
    "num_epoch = 100 #总共100批\n",
    "z_dimension = 200 #噪音维度\n",
    "input_dimension = 1000 #输入维度\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(155000, 1000) (210000, 1000)\n"
     ]
    }
   ],
   "source": [
    "spoof_data = np.load(\"/new_data/yhang/GNSS/PCA/PCA_ds8.npy\")\n",
    "clean_data = np.load(\"/new_data/yhang/GNSS/PCA/PCA_clean.npy\")\n",
    "print(spoof_data.shape, clean_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义转换器，数据转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#信号数据的处理过程\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m signal_transform \u001b[38;5;241m=\u001b[39m \u001b[43mtransforms\u001b[49m\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      3\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m      4\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.5\u001b[39m),std\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.5\u001b[39m))\n\u001b[1;32m      5\u001b[0m ])\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 加入标签，将相同类别的数据放在一起\u001b[39;00m\n\u001b[1;32m      8\u001b[0m real_data \u001b[38;5;241m=\u001b[39m clean_data\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#信号数据的处理过程\n",
    "signal_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5),std=(0.5))\n",
    "])\n",
    "\n",
    "# 加入标签，将相同类别的数据放在一起\n",
    "real_data = clean_data\n",
    "\n",
    "spoof_data = spoof_data\n",
    "\n",
    "# 创建标签，对于'real'类别，我们使用1\n",
    "real_labels = np.ones((real_data.shape[0],))\n",
    "\n",
    "# 对于'spoof'类别，我们使用0\n",
    "spoof_labels = np.zeros((spoof_data.shape[0],))\n",
    "\n",
    "# 将所有数据堆叠在一起\n",
    "data = np.vstack((real_data, spoof_data))\n",
    "\n",
    "# 同样，将所有标签堆叠在一起\n",
    "labels = np.concatenate((real_labels, spoof_labels))\n",
    "# 对标签进行维度修改改成(labels.shape[0], 1)\n",
    "labels = labels.reshape(labels.shape[0], 1)\n",
    "\n",
    "# 拆分数据为训练集和测试集\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size=0.3, random_state=42)\n",
    "np.save(\"/new_data/yhang/GNSS/TEXBAT/train/ds8/data.npy\", data_train)\n",
    "np.save(\"/new_data/yhang/GNSS/TEXBAT/train/ds8/label.npy\", labels_train)\n",
    "np.save(\"/new_data/yhang/GNSS/TEXBAT/test/ds8/data.npy\", data_test)\n",
    "np.save(\"/new_data/yhang/GNSS/TEXBAT/test/ds8/label.npy\", labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(255500, 1000) (255500, 1)\n"
     ]
    }
   ],
   "source": [
    "data_train = np.load(\"/new_data/yhang/GNSS/TEXBAT/train/ds8/data.npy\")\n",
    "labels_train = np.load(\"/new_data/yhang/GNSS/TEXBAT/train/ds8/label.npy\")\n",
    "print(data_train.shape, labels_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 制作成数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import torch.autograd\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "def normalize_features(data):\n",
    "    # 计算每个特征的平均值和标准差\n",
    "    mean = torch.mean(data, dim=0)\n",
    "    std = torch.std(data, dim=0)\n",
    "\n",
    "    # 避免除以0\n",
    "    std[std == 0] = 1\n",
    "\n",
    "    # 归一化数据\n",
    "    normalized_data = (data - mean) / std\n",
    "\n",
    "    return normalized_data\n",
    "\n",
    "# 将numpy数组转换为torch张量\n",
    "data_torch = torch.from_numpy(data_train).float()\n",
    "data_torch = normalize_features(data_torch)\n",
    "labels_torch = torch.from_numpy(labels_train).float()\n",
    "\n",
    "# 创建一个TensorDataset\n",
    "dataset = TensorDataset(data_torch, labels_torch)\n",
    "\n",
    "# 创建一个DataLoader\n",
    "batch_size = batch_size  # 你可以根据需要调整这个值\n",
    "dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = True, drop_last = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义判别器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将信号28x28展开成784，然后通过多层感知器，中间经过斜率设置为0.2的LeakyReLU激活函数，\n",
    "# 最后接sigmoid激活函数得到一个0到1之间的概率进行二分类。\n",
    "class discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(discriminator,self).__init__()\n",
    "        self.dis = nn.Sequential(\n",
    "            nn.Linear(input_dimension,256),#输入特征数为1000，输出为256\n",
    "            nn.LeakyReLU(0.2),#进行非线性映射\n",
    "            nn.Linear(256,256),#进行一个线性映射\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256,1),\n",
    "            nn.Sigmoid()#也是一个激活函数，二分类问题中，\n",
    "            # sigmoid可以班实数映射到【0,1】，作为概率值，\n",
    "            # 多分类用softmax函数\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.dis(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#输入一个100维的0～1之间的高斯分布，然后通过第一层线性变换将其映射到256维,\n",
    "# 然后通过LeakyReLU激活函数，接着进行一个线性变换，再经过一个LeakyReLU激活函数，\n",
    "# 然后经过线性变换将其变成784维，最后经过Tanh激活函数是希望生成的假的信号数据分布\n",
    "# 能够在-1～1之间。\n",
    "class generator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Linear(z_dimension, 256), #用线性变换将输入映射到256维\n",
    "            nn.ReLU(True),       #relu激活\n",
    "            nn.Linear(256, 256), #线性变换\n",
    "            nn.ReLU(True),       #relu激活\n",
    "            nn.Linear(256, input_dimension), #线性变换\n",
    "            nn.Tanh()            #Tanh激活使得生成数据分布在【-1,1】之间\n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.gen(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建对象\n",
    "D = discriminator()\n",
    "G = generator()\n",
    "if torch.cuda.is_available():\n",
    "    D = D.to(device)\n",
    "    G = G.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########判别器训练train#####################\n",
    "#分为两部分：1、真的图像判别为真；2、假的图像判别为假\n",
    "#此过程中，生成器参数不断更新\n",
    " \n",
    "#首先需要定义loss的度量方式  （二分类的交叉熵）\n",
    "#其次定义 优化函数,优化函数的学习率为0.0003\n",
    "criterion = nn.BCELoss() #是单目标二分类交叉熵函数\n",
    "d_optimizer = torch.optim.Adam(D.parameters(),lr=0.0003)\n",
    "g_optimizer = torch.optim.Adam(G.parameters(),lr=0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################进入训练##判别器的判断过程#####################\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# 创建一个SummaryWriter实例\n",
    "writer = SummaryWriter(\"/home/yhang/GAN/gan-torch/board/tensorboard/GAN_TEXBAT_ds8\")\n",
    "\n",
    "update_times = 0\n",
    "\n",
    "for epoch in range(4 * num_epoch): #进行多个epoch的训练\n",
    "    for i,(signals, labels) in enumerate(dataloader):\n",
    "        num_signals = signals.size(0)\n",
    "        \n",
    "        # view()函数作用是将一个多行的Tensor,拼接成一行\n",
    "        # 第一个参数是要拼接的tensor,第二个参数是-1\n",
    "        # =============================训练判别器==================\n",
    "        signals = signals.view(num_signals, -1)  # 将信号展成1000维的向量\n",
    "        real_signals = signals.to(device)  # 将tensor变成Variable放入计算图中\n",
    "        \n",
    "        real_label = labels.to(device)  # 已有的信号的lable已经放在dataloader里面了\n",
    "\n",
    "\n",
    "        # 计算信号的损失\n",
    "        real_out = D(real_signals)  # 将数据集信号放入判别器中\n",
    "        # print(\"real_label.size\", real_label.size())\n",
    "        d_loss_real = criterion(real_out, real_label).to(device)  # 得到数据集的loss\n",
    "\n",
    "        # 损失函数和优化\n",
    "        d_loss = d_loss_real  #损失包括判真损失和判假损失\n",
    "        d_optimizer.zero_grad()  # 在反向传播之前，先将梯度归0\n",
    "        d_loss.backward()  # 将误差反向传播\n",
    "        d_optimizer.step()  # 更新参数\n",
    "\n",
    "        # 将损失值添加到TensorBoard\n",
    "        writer.add_scalar('Loss/train_ds8_mlp', d_loss, update_times)\n",
    "        update_times += 1\n",
    "\n",
    "writer.close()   \n",
    " \n",
    "        # if epoch == 0:\n",
    "        #     real_images=to_img(real_img.cpu().data)\n",
    "        #     save_image(real_images, './img/real_images.png')\n",
    " \n",
    "        # fake_images = to_img(real_img.cpu().data)\n",
    "        # save_image(fake_images, './img/fake_images-{}.png'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(D.state_dict(), '/home/yhang/GAN/gan-torch/model/ds8_D_model0.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试测试集上模型的表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109500, 1000) (109500, 1)\n"
     ]
    }
   ],
   "source": [
    "data_test = np.load(\"/new_data/yhang/GNSS/TEXBAT/test/ds8/data.npy\")\n",
    "labels_test = np.load(\"/new_data/yhang/GNSS/TEXBAT/test/ds8/label.npy\")\n",
    "print(data_test.shape, labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = discriminator().to(device)\n",
    "D.load_state_dict(torch.load('/home/yhang/GAN/gan-torch/model/ds8_D_model0.pth'))\n",
    "# 将numpy数组转换为torch张量\n",
    "data_torch = torch.from_numpy(data_test).float()\n",
    "data_torch = normalize_features(data_torch)\n",
    "labels_torch = torch.from_numpy(labels_test).float()\n",
    "\n",
    "# 创建一个TensorDataset\n",
    "dataset = TensorDataset(data_torch, labels_torch)\n",
    "\n",
    "# 创建一个DataLoader\n",
    "batch_size = batch_size  # 你可以根据需要调整这个值\n",
    "dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = True, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test data: 58.345212 %\n"
     ]
    }
   ],
   "source": [
    "D.eval()  # 设置模型为评估模式\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # 在评估模式下，我们不需要计算梯度\n",
    "    for data, labels in dataloader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        outputs = D(data)\n",
    "        predicted = (outputs > 0.5).float()  # 使用0.5作为阈值来获取预测的标签\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print('Accuracy of the network on the test data: %f %%' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "###########################进入训练##判别器的判断过程#####################\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# 创建一个SummaryWriter实例\n",
    "writer = SummaryWriter(\"/home/yhang/GAN/gan-torch/tensorboard/GAN_TEXBAT_ds8\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "update_times = 0\n",
    "for epoch in range(num_epoch): #进行多个epoch的训练\n",
    "    for i,(signals, labels) in enumerate(dataloader):\n",
    "        num_signals = signals.size(0)\n",
    "        \n",
    "        # view()函数作用是将一个多行的Tensor,拼接成一行\n",
    "        # 第一个参数是要拼接的tensor,第二个参数是-1\n",
    "        # =============================训练判别器==================\n",
    "        signals = signals.view(num_signals, -1)  # 将信号展成1000维的向量\n",
    "        real_signals = signals.to(device)  # 将tensor变成Variable放入计算图中\n",
    "        \n",
    "        real_label = labels.to(device)  # 已有的信号的lable已经放在dataloader里面了\n",
    "        fake_label = torch.zeros(num_signals).reshape(-1, 1).to(device)  # 定义假的信号的label为0\n",
    "        true_label = torch.ones(num_signals).reshape(-1, 1).to(device)  # 定义真实的信号的label为1\n",
    "\n",
    "        # 计算信号的损失\n",
    "        real_out = D(real_signals)  # 将数据集信号放入判别器中\n",
    "        # print(\"real_label.size\", real_label.size())\n",
    "        d_loss_real = criterion(real_out, real_label).to(device)  # 得到数据集的loss\n",
    "        real_scores = real_out  # 得到真实信号的判别值，输出的值越接近1越好\n",
    " \n",
    "        # 计算假的信号的损失\n",
    "        z = Variable(torch.randn(num_signals, z_dimension)).to(device)  # 随机生成一些噪声\n",
    "        fake_signals = G(z)  # 随机噪声放入生成网络中，生成一张假的信号\n",
    "        fake_out = D(fake_signals)  # 判别器判断假的信号\n",
    "        d_loss_fake = criterion(fake_out, fake_label).to(device)  # 得到假的信号的loss\n",
    "        fake_scores = fake_out  # 得到假信号的判别值，对于判别器来说，假信号的损失越接近0越好\n",
    " \n",
    "        # 损失函数和优化\n",
    "        d_loss = d_loss_real + d_loss_fake #损失包括判真损失和判假损失\n",
    "        d_optimizer.zero_grad()  # 在反向传播之前，先将梯度归0\n",
    "        d_loss.backward()  # 将误差反向传播\n",
    "        d_optimizer.step()  # 更新参数\n",
    "        writer.add_scalar('discriminator_Loss', d_loss, update_times)\n",
    "        # ==================训练生成器============================\n",
    "        ################################生成网络的训练###############################\n",
    "        # 原理：目的是希望生成的假的信号被判别器判断为真的信号，\n",
    "        # 在此过程中，将判别器固定，将假的信号传入判别器的结果与真实的label对应，\n",
    "        # 反向传播更新的参数是生成网络里面的参数，\n",
    "        # 这样可以通过更新生成网络里面的参数，来训练网络，使得生成的信号让判别器以为是真的\n",
    "        # 这样就达到了对抗的目的\n",
    " \n",
    "        # 计算假的信号的损失\n",
    " \n",
    "        z = torch.randn(num_signals, z_dimension)  # 得到随机噪声\n",
    "        z = z.to(device)\n",
    "        fake_signals = G(z) #随机噪声输入到生成器中，得到假的信号\n",
    "        output = D(fake_signals)  # 经过判别器得到的结果\n",
    "        g_loss = criterion(output, true_label).to(device)  # 得到的假的信号与真实的信号的label的loss\n",
    " \n",
    "        # bp and optimize\n",
    "        g_optimizer.zero_grad()  # 梯度归0\n",
    "        g_loss.backward()  # 进行反向传播\n",
    "        g_optimizer.step()  # .step()一般用在反向传播后面,用于更新生成网络的参数\n",
    " \n",
    "        writer.add_scalar('generator_Loss', g_loss, update_times)\n",
    "        update_times += 1\n",
    " \n",
    "        # if epoch == 0:\n",
    "        #     real_images=to_img(real_img.cpu().data)\n",
    "        #     save_image(real_images, './img/real_images.png')\n",
    " \n",
    "        # fake_images = to_img(real_img.cpu().data)\n",
    "        # save_image(fake_images, './img/fake_images-{}.png'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "torch.save(G.state_dict(), '/home/yhang/GAN/gan-torch/model/raw_G_model_ds8.pth')\n",
    "torch.save(D.state_dict(), '/home/yhang/GAN/gan-torch/model/raw_D_model_ds8.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109500, 1000) (109500, 1)\n",
      "Accuracy of the network on the test data: 58.490497 %\n"
     ]
    }
   ],
   "source": [
    "data_test = np.load(\"/new_data/yhang/GNSS/TEXBAT/test/ds8/data.npy\")\n",
    "labels_test = np.load(\"/new_data/yhang/GNSS/TEXBAT/test/ds8/label.npy\")\n",
    "print(data_test.shape, labels_test.shape)\n",
    "\n",
    "D = discriminator().to(device)\n",
    "D.load_state_dict(torch.load(\"/home/yhang/GAN/gan-torch/model/raw_D_model_ds8.pth\"))\n",
    "# 将numpy数组转换为torch张量\n",
    "data_torch = torch.from_numpy(data_test).float()\n",
    "data_torch = normalize_features(data_torch)\n",
    "labels_torch = torch.from_numpy(labels_test).float()\n",
    "\n",
    "# 创建一个TensorDataset\n",
    "dataset = TensorDataset(data_torch, labels_torch)\n",
    "\n",
    "# 创建一个DataLoader\n",
    "batch_size = batch_size  # 你可以根据需要调整这个值\n",
    "dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = True, drop_last = True)\n",
    "\n",
    "D.eval()  # 设置模型为评估模式\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # 在评估模式下，我们不需要计算梯度\n",
    "    for data, labels in dataloader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        outputs = D(data)\n",
    "        predicted = (outputs > 0.5).float()  # 使用0.5作为阈值来获取预测的标签\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print('Accuracy of the network on the test data: %f %%' % accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNSS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
